{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-10T21:12:39.030528Z",
     "iopub.status.busy": "2023-03-10T21:12:39.030290Z",
     "iopub.status.idle": "2023-03-10T21:12:41.112668Z",
     "shell.execute_reply": "2023-03-10T21:12:41.112007Z",
     "shell.execute_reply.started": "2023-03-10T21:12:39.030509Z"
    },
    "id": "5JKFOepBjjII"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Activation, Conv2D\n",
    "from keras.layers import MaxPooling2D, Dropout, Flatten,GlobalMaxPooling2D\n",
    "from keras.models import Sequential\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import BatchNormalization\n",
    "# from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D\n",
    "# from keras.layers import LeakyReLU \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-10T21:12:41.114430Z",
     "iopub.status.busy": "2023-03-10T21:12:41.113826Z",
     "iopub.status.idle": "2023-03-10T21:12:42.338342Z",
     "shell.execute_reply": "2023-03-10T21:12:42.337533Z",
     "shell.execute_reply.started": "2023-03-10T21:12:41.114408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1282 files belonging to 2 classes.\n",
      "Using 1026 files for training.\n",
      "Found 1282 files belonging to 2 classes.\n",
      "Using 256 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_dir=\"/notebooks/thermograms/Desenvolvimento da Metodologia\"\n",
    "validation_dir=train_dir\n",
    "batch_size=16\n",
    "test_train_split=0.2\n",
    "train_data = image_dataset_from_directory(\\\n",
    "      train_dir,color_mode=\"grayscale\",image_size=(227,227) ,\\\n",
    "      subset='training',seed=12, validation_split=test_train_split,\\\n",
    "      batch_size=batch_size)\n",
    "validation_data = image_dataset_from_directory(validation_dir,\n",
    "      color_mode=\"grayscale\",image_size=(227,227), subset='validation',seed=12,\\\n",
    "      validation_split=test_train_split,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-10T21:12:42.339331Z",
     "iopub.status.busy": "2023-03-10T21:12:42.339187Z",
     "iopub.status.idle": "2023-03-10T21:12:42.345294Z",
     "shell.execute_reply": "2023-03-10T21:12:42.344559Z",
     "shell.execute_reply.started": "2023-03-10T21:12:42.339314Z"
    },
    "id": "R8-cow2LjlLo"
   },
   "outputs": [],
   "source": [
    "def build_model(layer_dims, input_shape=(227,227,3,),len_classes=3, dropout_rate=0.2,activation='relu'):\n",
    "    \"\"\"Function to build a model with specified layer dimensions and activation function.\"\"\"\n",
    "    model = Sequential()\n",
    "    for i, dim in enumerate(layer_dims):\n",
    "        if i == 0:\n",
    "            model.add(Conv2D(dim[0], dim[1], input_shape=input_shape, activation=activation))\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        else:\n",
    "            model.add(Conv2D(dim[0], dim[1], activation=activation))\n",
    "            if True:#i%2!=0:\n",
    "                model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        #model.add(Conv2D(filters=, kernel_size=1, strides=1))\n",
    "        #model.add(Dropout(dropout_rate))\n",
    "        # model.add(BatchNormalization())\n",
    "        #model.add(GlobalMaxPooling2D())\n",
    "        #model.add(Activation('sigmoid'))\n",
    "    # model.add(Dropout(dropout_rate))\n",
    "    # Output Layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50))\n",
    "    model.add(Activation('relu'))\n",
    "    # Add Dropout\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(len_classes-1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-10T21:12:42.346325Z",
     "iopub.status.busy": "2023-03-10T21:12:42.346163Z",
     "iopub.status.idle": "2023-03-10T21:12:42.349931Z",
     "shell.execute_reply": "2023-03-10T21:12:42.349413Z",
     "shell.execute_reply.started": "2023-03-10T21:12:42.346280Z"
    },
    "id": "WMfe0TL_joQp"
   },
   "outputs": [],
   "source": [
    "def sample_architecture(min_layers=1, max_layers=5, min_filters=32, max_filters=512, min_kernel=3, max_kernel=5):\n",
    "    \"\"\"Function to sample a random architecture from the search space.\"\"\"\n",
    "    num_layers = random.randint(min_layers, max_layers)\n",
    "    layer_dims = [(random.randint(min_filters, max_filters), random.randint(min_kernel, max_kernel)) for _ in range(num_layers)]\n",
    "    return layer_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-10T21:12:42.351482Z",
     "iopub.status.busy": "2023-03-10T21:12:42.351309Z",
     "iopub.status.idle": "2023-03-10T21:12:42.355022Z",
     "shell.execute_reply": "2023-03-10T21:12:42.354542Z",
     "shell.execute_reply.started": "2023-03-10T21:12:42.351467Z"
    },
    "id": "vekdUB7jjq2S"
   },
   "outputs": [],
   "source": [
    "def mutate_architecture(layer_dims, mutation_rate=0.1, min_layers=1, max_layers=5, min_filters=32, max_filters=512, min_kernel=3, max_kernel=5):\n",
    "    \"\"\"Function to mutate an architecture by randomly modifying some of its layer dimensions.\"\"\"\n",
    "    num_layers = len(layer_dims)\n",
    "    for i in range(num_layers):\n",
    "        if np.random.rand() < mutation_rate:\n",
    "            layer_dims[i] = (random.randint(min_filters, max_filters), random.randint(min_kernel, max_kernel))\n",
    "    return layer_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-10T21:12:42.355943Z",
     "iopub.status.busy": "2023-03-10T21:12:42.355806Z",
     "iopub.status.idle": "2023-03-10T21:12:42.359509Z",
     "shell.execute_reply": "2023-03-10T21:12:42.358992Z",
     "shell.execute_reply.started": "2023-03-10T21:12:42.355940Z"
    },
    "id": "6-jGT9t8RZsa"
   },
   "outputs": [],
   "source": [
    "def breed_architectures(parent1, parent2, mutation_rate, min_layers, max_layers, min_filters, max_filters, min_kernel, max_kernel):\n",
    "    \"\"\"Function to breed two parent architectures to produce a child architecture.\"\"\"\n",
    "    child = []\n",
    "    for i in range(len(parent1)):\n",
    "        if np.random.rand() < 0.5:\n",
    "            child.append(parent1[i])\n",
    "        else:\n",
    "            if i < len(parent2):\n",
    "                child.append(parent2[i])\n",
    "    return mutate_architecture(child, mutation_rate, min_layers, max_layers, min_filters, max_filters, min_kernel, max_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-10T21:12:42.360279Z",
     "iopub.status.busy": "2023-03-10T21:12:42.360120Z",
     "iopub.status.idle": "2023-03-10T21:12:42.364680Z",
     "shell.execute_reply": "2023-03-10T21:12:42.364201Z",
     "shell.execute_reply.started": "2023-03-10T21:12:42.360263Z"
    },
    "id": "lbnz3OeokBgR"
   },
   "outputs": [],
   "source": [
    "#Train the model on the dataset and evaluate its performance.\n",
    "def train_and_evaluate_model(model,epochs=10, train_dir=None,X_train=None, y_train=None,\\\n",
    "                             X_test=None,y_test=None):\n",
    "    \"\"\"Function to train and evaluate a model.\"\"\"\n",
    "    if train_dir is not None:\n",
    "        #train_data,validation_data=get_data(train_dir,train_dir)\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer='Adam', metrics=[\"BinaryAccuracy\"])\n",
    "        # print(model.summary())\n",
    "        history = model.fit(train_data, epochs=epochs, verbose=1,validation_data=validation_data)\n",
    "        # return history\n",
    "        # print(history.history.keys())\n",
    "        # Extract the accuracy from the history object\n",
    "        acc = history.history['val_binary_accuracy'][len(history.history['val_binary_accuracy'])-1]\n",
    "        return acc\n",
    "    else:\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer='Adam', metrics=[\"BinaryAccuracy\"])\n",
    "        model.fit(X_train, y_train, epochs=epochs, batch_size=32, verbose=1)\n",
    "        scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "        return scores[1]  # Return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-10T21:12:42.365611Z",
     "iopub.status.busy": "2023-03-10T21:12:42.365450Z",
     "iopub.status.idle": "2023-03-10T21:12:42.374193Z",
     "shell.execute_reply": "2023-03-10T21:12:42.373552Z",
     "shell.execute_reply.started": "2023-03-10T21:12:42.365611Z"
    },
    "id": "2zwUTo9ykIp7"
   },
   "outputs": [],
   "source": [
    "def genetic_algorithm(train_dir, epochs, population_size=20, len_classes=3, num_generations=10, mutation_rate=0.1,\\\n",
    "                      min_layers=1, max_layers=5, min_filters=32, max_filters=512,\\\n",
    "                      min_kernel=3, max_kernel=5, save_after=10,checkpoint_file=None):\n",
    "    \"\"\"Function to implement the genetic algorithm to evolve the architecture.\"\"\"\n",
    "    if checkpoint_file and os.path.exists(checkpoint_file):\n",
    "        with open(checkpoint_file, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            population = data['population']\n",
    "            scores = data['scores']\n",
    "            start_generation = data['generation']\n",
    "            start_population = data['population_id']\n",
    "    else:\n",
    "        # Initialize population\n",
    "        population = [sample_architecture(min_layers, max_layers, min_filters,\\\n",
    "                                          max_filters,min_kernel, max_kernel)\\\n",
    "                      for _ in range(population_size)]\n",
    "        scores = []\n",
    "        start_generation = 0\n",
    "        start_population = 0\n",
    "    \n",
    "    for i in range(start_generation, num_generations):\n",
    "        print(\"Generation {}/{}\".format(i + 1, num_generations))\n",
    "        new_population = []\n",
    "        for j in range(population_size):\n",
    "            # Select two random parents\n",
    "            the_choice=np.random.choice(population_size, 2, replace=False)\n",
    "            parent1=population[the_choice[0]]\n",
    "            parent2=population[the_choice[1]]\n",
    "            # Breed the parents to produce a child\n",
    "            child = breed_architectures(parent1, parent2, mutation_rate,\\\n",
    "                                        min_layers,max_layers, min_filters,\\\n",
    "                                        max_filters, min_kernel, max_kernel)\n",
    "            new_population.append(child)\n",
    "            # Checkpoint after every 5th model trained\n",
    "            if checkpoint_file and j % save_after == save_after-1:\n",
    "                data = {\n",
    "                    'population': population,\n",
    "                    'scores': scores,\n",
    "                    'generation': i,\n",
    "                    'population_id': start_population+j+1\n",
    "                }\n",
    "                with open(checkpoint_file, 'wb') as f:\n",
    "                    pickle.dump(data, f)\n",
    "        # Evaluate each model in the population\n",
    "        generation_scores = []\n",
    "        for j, layer_dims in enumerate(new_population):\n",
    "            print(str(j)+\"/\"+str(population_size))            \n",
    "            model = build_model(layer_dims,len_classes=len_classes,input_shape=(227,227,1,))\n",
    "            score = train_and_evaluate_model(model,epochs=epochs,train_dir=train_dir)\n",
    "            generation_scores.append((start_population+j, layer_dims, score))\n",
    "            # Checkpoint after each model trained\n",
    "            if checkpoint_file:\n",
    "                data = {\n",
    "                    'population': population,\n",
    "                    'scores': scores + generation_scores[-5:],\n",
    "                    'generation': i,\n",
    "                    'population_id': start_population+j+1\n",
    "                }\n",
    "                with open(checkpoint_file, 'wb') as f:\n",
    "                    pickle.dump(data, f)\n",
    "        # Sort the models by accuracy and select the best ones for the next generation\n",
    "        generation_scores.sort(key=lambda x: x[2], reverse=True)\n",
    "        population = [x[1] for x in generation_scores[:population_size]]\n",
    "        scores.extend(generation_scores)\n",
    "        # Save checkpoint after each generation\n",
    "        if checkpoint_file:\n",
    "            data = {\n",
    "                'population': population,\n",
    "                'scores': scores,\n",
    "                'generation': i,\n",
    "                'population_id': start_population+population_size\n",
    "            }\n",
    "            with open(checkpoint_file, 'wb') as f:\n",
    "                pickle.dump(data, f)\n",
    "\n",
    "    # Return the best architecture\n",
    "    return population[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-10T21:12:42.375089Z",
     "iopub.status.busy": "2023-03-10T21:12:42.374914Z"
    },
    "id": "1fn6mVEWSatF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 3/100\n",
      "0/50\n",
      "Epoch 1/7\n",
      "65/65 [==============================] - 19s 197ms/step - loss: 2.1413 - binary_accuracy: 0.8519 - val_loss: 0.1944 - val_binary_accuracy: 0.9297\n",
      "Epoch 2/7\n",
      "65/65 [==============================] - 12s 183ms/step - loss: 0.1282 - binary_accuracy: 0.9542 - val_loss: 0.1222 - val_binary_accuracy: 0.9766\n",
      "Epoch 3/7\n",
      "65/65 [==============================] - 12s 183ms/step - loss: 0.0579 - binary_accuracy: 0.9786 - val_loss: 0.1288 - val_binary_accuracy: 0.9805\n",
      "Epoch 4/7\n",
      "65/65 [==============================] - 12s 184ms/step - loss: 0.0376 - binary_accuracy: 0.9854 - val_loss: 0.1401 - val_binary_accuracy: 0.9844\n",
      "Epoch 5/7\n",
      "65/65 [==============================] - 12s 182ms/step - loss: 0.0306 - binary_accuracy: 0.9883 - val_loss: 0.1598 - val_binary_accuracy: 0.9688\n",
      "Epoch 6/7\n",
      "65/65 [==============================] - 12s 183ms/step - loss: 0.0837 - binary_accuracy: 0.9795 - val_loss: 0.1150 - val_binary_accuracy: 0.9727\n",
      "Epoch 7/7\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0126 - binary_accuracy: 0.9951"
     ]
    }
   ],
   "source": [
    "best_architecture2 = genetic_algorithm(train_dir=train_dir,len_classes=2,epochs=7,population_size=50, num_generations=100, mutation_rate=0.1,\\\n",
    "                      min_layers=1, max_layers=5, min_filters=32, max_filters=512,\\\n",
    "                      min_kernel=3, max_kernel=5, checkpoint_file='/notebooks/hnas_major/models/checkpoint_file.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model2=build_model(best_architecture2,len_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model2.compile(loss=\"binary_crossentropy\", optimizer='Adam', metrics=[\"BinaryAccuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model2.summary()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Dense, Activation, Conv2D\n",
        "from keras.layers import MaxPooling2D, Dropout, Flatten,GlobalMaxPooling2D\n",
        "from keras.models import Sequential\n",
        "import random\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import BatchNormalization\n",
        "# from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D\n",
        "# from keras.layers import LeakyReLU \n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory"
      ],
      "metadata": {
        "id": "5JKFOepBjjII"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(layer_dims, input_shape=(256,256,3,),len_classes=2, dropout_rate=0.2,activation='relu'):\n",
        "    print(1)\n",
        "    \"\"\"Function to build a model with specified layer dimensions and activation function.\"\"\"\n",
        "    model = Sequential()\n",
        "    for i, dim in enumerate(layer_dims):\n",
        "        if i == 0:\n",
        "            model.add(Conv2D(dim[0], dim[1], input_shape=input_shape, activation=activation))\n",
        "            model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "        else:\n",
        "            model.add(Conv2D(dim[0], dim[1], activation=activation))\n",
        "            model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Conv2D(filters=len_classes, kernel_size=1, strides=1))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(GlobalMaxPooling2D())\n",
        "    model.add(Activation('softmax'))\n",
        "    return model"
      ],
      "metadata": {
        "id": "R8-cow2LjlLo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_architecture(min_layers=1, max_layers=5, min_filters=32, max_filters=512, min_kernel=3, max_kernel=5):\n",
        "    \"\"\"Function to sample a random architecture from the search space.\"\"\"\n",
        "    num_layers = random.randint(min_layers, max_layers)\n",
        "    layer_dims = [(random.randint(min_filters, max_filters), random.randint(min_kernel, max_kernel)) for _ in range(num_layers)]\n",
        "    return layer_dims"
      ],
      "metadata": {
        "id": "WMfe0TL_joQp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mutate_architecture(layer_dims, mutation_rate=0.1, min_layers=1, max_layers=5, min_filters=32, max_filters=512, min_kernel=3, max_kernel=5):\n",
        "    \"\"\"Function to mutate an architecture by randomly modifying some of its layer dimensions.\"\"\"\n",
        "    num_layers = len(layer_dims)\n",
        "    for i in range(num_layers):\n",
        "        if np.random.rand() < mutation_rate:\n",
        "            layer_dims[i] = (random.randint(min_filters, max_filters), random.randint(min_kernel, max_kernel))\n",
        "    return layer_dims"
      ],
      "metadata": {
        "id": "vekdUB7jjq2S"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def breed_architectures(parent1, parent2, mutation_rate=0.1, min_layers=1, max_layers=5, min_filters=32, max_filters=512, min_kernel=3, max_kernel=5):\n",
        "#     \"\"\"Function to breed two architectures to produce a new architecture.\"\"\"\n",
        "#     child = []\n",
        "#     for i in range(len(parent1)):\n",
        "#         if np.random.rand() < 0.5:\n",
        "#             child.append(parent1[i])\n",
        "#         else:\n",
        "#             child.append(parent2[i])\n",
        "#     return mutate_architecture(child, mutation_rate, min_layers, max_layers, \\\n",
        "#                                min_filters, max_filters, min_kernel, max_kernel)\n"
      ],
      "metadata": {
        "id": "OBesHxjejvVo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def breed_architectures(parent1, parent2, mutation_rate, min_layers, max_layers, min_filters, max_filters, min_kernel, max_kernel):\n",
        "    \"\"\"Function to breed two parent architectures to produce a child architecture.\"\"\"\n",
        "    child = []\n",
        "    for i in range(len(parent1)):\n",
        "        if np.random.rand() < 0.5:\n",
        "            child.append(parent1[i])\n",
        "        else:\n",
        "            if i < len(parent2):\n",
        "                child.append(parent2[i])\n",
        "    return mutate_architecture(child, mutation_rate, min_layers, max_layers, min_filters, max_filters, min_kernel, max_kernel)\n"
      ],
      "metadata": {
        "id": "6-jGT9t8RZsa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(train_dir,validation_dir,batch_size=60,test_train_split=0.2):\n",
        "    train_data = image_dataset_from_directory(\\\n",
        "      train_dir,color_mode=\"grayscale\",image_size=(256,256) ,\\\n",
        "      subset='training',seed=12, validation_split=test_train_split,\\\n",
        "      batch_size=batch_size)\n",
        "    validation_data = image_dataset_from_directory(validation_dir,\n",
        "      color_mode=\"grayscale\",image_size=(256,256), subset='validation',seed=12,\\\n",
        "      validation_split=test_train_split,batch_size=batch_size)\n",
        "    return train_data,validation_data"
      ],
      "metadata": {
        "id": "UiCgtQa9-62a"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model on the dataset and evaluate its performance.\n",
        "def train_and_evaluate_model(model, train_dir=None,X_train=None, y_train=None,\\\n",
        "                             X_test=None,y_test=None):\n",
        "    \"\"\"Function to train and evaluate a model.\"\"\"\n",
        "    if train_dir is not None:\n",
        "        train_data,validation_data=get_data(train_dir,train_dir)\n",
        "        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "        history = model.fit(train_data, epochs=10, verbose=0)\n",
        "        # Extract the accuracy from the history object\n",
        "        acc = history.history['val_acc'] if 'val_acc' in history.history else history.history['acc']\n",
        "        return acc\n",
        "    else:\n",
        "        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "        model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
        "        scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "        return scores[1]  # Return accuracy"
      ],
      "metadata": {
        "id": "lbnz3OeokBgR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Define the genetic algorithm to evolve the architecture.\n",
        "def genetic_algorithm(train_dir,population_size=20, num_generations=10, mutation_rate=0.1,\\\n",
        "                      min_layers=1, max_layers=5, min_filters=32, max_filters=512,\\\n",
        "                      min_kernel=3, max_kernel=5):\n",
        "    \"\"\"Function to implement the genetic algorithm to evolve the architecture.\"\"\"\n",
        "    # Initialize population\n",
        "    population = [sample_architecture(min_layers, max_layers, min_filters,\\\n",
        "                                      max_filters,min_kernel, max_kernel)\\\n",
        "                  for _ in range(population_size)]\n",
        "    scores = []\n",
        "    for i in range(num_generations):\n",
        "        print(\"Generation {}/{}\".format(i + 1, num_generations))\n",
        "        new_population = []\n",
        "        for j in range(population_size):\n",
        "            # Select two random parents\n",
        "            the_choice=np.random.choice(population_size, 2, replace=False)\n",
        "            parent1=population[the_choice[0]]\n",
        "            parent2=population[the_choice[1]]\n",
        "            # Breed the parents to produce a child\n",
        "            child = breed_architectures(parent1, parent2, mutation_rate,\\\n",
        "                                        min_layers,max_layers, min_filters,\\\n",
        "                                        max_filters, min_kernel, max_kernel)\n",
        "            new_population.append(child)\n",
        "        # Evaluate each model in the population\n",
        "        scores = []\n",
        "        for j, layer_dims in enumerate(new_population):\n",
        "            model = build_model(layer_dims,input_shape=(256,256,1,))\n",
        "            # (X_train, y_train), (X_test, y_test)=tf.keras.datasets.mnist.load_data()\n",
        "            score = train_and_evaluate_model(model,train_dir=\"/content/drive/MyDrive/cbis_ddsm/mass/train\")\n",
        "#  X_train, y_train, X_test, y_test)\n",
        "            scores.append((layer_dims, score))\n",
        "        # Sort the models by accuracy and select the best ones for the next generation\n",
        "        scores.sort(key=lambda x: x[1], reverse=True)\n",
        "        population = [x[0] for x in scores[:population_size]]\n",
        "    # Return the best architecture\n",
        "    return population[0]"
      ],
      "metadata": {
        "id": "2zwUTo9ykIp7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir=\"/content/drive/MyDrive/cbis_ddsm/mass/train\"\n",
        "test_dir=\"/content/drive/MyDrive/cbis_ddsm/mass/test\""
      ],
      "metadata": {
        "id": "1fn6mVEWSatF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "J5rKCnQajbbI",
        "outputId": "7ef537d1-5197-44a4-d0f6-25104d0aa12c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation 1/10\n",
            "1\n",
            "Found 552 files belonging to 3 classes.\n",
            "Using 442 files for training.\n",
            "Found 552 files belonging to 3 classes.\n",
            "Using 110 files for validation.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-eb184e61bdad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# input_shape = (200, 100,3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Run the genetic algorithm to evolve the architecture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbest_architecture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenetic_algorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-ff3e67016ce6>\u001b[0m in \u001b[0;36mgenetic_algorithm\u001b[0;34m(train_dir, population_size, num_generations, mutation_rate, min_layers, max_layers, min_filters, max_filters, min_kernel, max_kernel)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_dims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m# (X_train, y_train), (X_test, y_test)=tf.keras.datasets.mnist.load_data()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/cbis_ddsm/mass/train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;31m#  X_train, y_train, X_test, y_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-52a4630da234>\u001b[0m in \u001b[0;36mtrain_and_evaluate_model\u001b[0;34m(model, train_dir, X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;31m# Extract the accuracy from the history object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'val_acc'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'gradient_tape/sequential/max_pooling2d/MaxPool/MaxPoolGrad' defined at (most recent call last):\n    File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py\", line 612, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py\", line 149, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/ioloop.py\", line 690, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/ioloop.py\", line 743, in _run_callback\n      ret = callback()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 787, in inner\n      self.run()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 748, in run\n      yielded = self.gen.send(value)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 381, in dispatch_queue\n      yield self.process_one()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 225, in wrapper\n      runner = Runner(result, future, yielded)\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 714, in __init__\n      self.run()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 748, in run\n      yielded = self.gen.send(value)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 365, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 209, in wrapper\n      yielded = next(result)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 209, in wrapper\n      yielded = next(result)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 543, in execute_request\n      self.do_execute(\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 209, in wrapper\n      yielded = next(result)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 306, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2854, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2881, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3057, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3249, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-11-eb184e61bdad>\", line 3, in <module>\n      best_architecture = genetic_algorithm(train_dir=train_dir)\n    File \"<ipython-input-9-ff3e67016ce6>\", line 29, in genetic_algorithm\n      score = train_and_evaluate_model(model,train_dir=\"/content/drive/MyDrive/cbis_ddsm/mass/train\")\n    File \"<ipython-input-8-52a4630da234>\", line 8, in train_and_evaluate_model\n      history = model.fit(train_data, epochs=10, verbose=0)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1409, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1051, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1040, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1030, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 893, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\", line 537, in minimize\n      grads_and_vars = self._compute_gradients(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\", line 590, in _compute_gradients\n      grads_and_vars = self._get_gradients(tape, loss, var_list, grad_loss)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\", line 471, in _get_gradients\n      grads = tape.gradient(loss, var_list, grad_loss)\nNode: 'gradient_tape/sequential/max_pooling2d/MaxPool/MaxPoolGrad'\nOOM when allocating tensor with shape[60,310,253,253] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node gradient_tape/sequential/max_pooling2d/MaxPool/MaxPoolGrad}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_1126]"
          ]
        }
      ],
      "source": [
        "# input_shape = (200, 100,3) \n",
        "# Run the genetic algorithm to evolve the architecture\n",
        "best_architecture = genetic_algorithm(train_dir=train_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6rtTbPp1jdY7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
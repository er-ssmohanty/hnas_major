{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5JKFOepBjjII"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 13:30:45.458675: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-06 13:30:45.953592: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-06 13:30:45.953680: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-06 13:30:49.245709: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-06 13:30:49.256270: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-06 13:30:49.256331: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Activation, Conv2D\n",
    "from keras.layers import MaxPooling2D, Dropout, Flatten,GlobalMaxPooling2D\n",
    "from keras.models import Sequential\n",
    "import random\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import BatchNormalization\n",
    "# from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D\n",
    "# from keras.layers import LeakyReLU \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "R8-cow2LjlLo"
   },
   "outputs": [],
   "source": [
    "def build_model(layer_dims, input_shape=(256,256,3,),len_classes=3, dropout_rate=0.2,activation='relu'):\n",
    "    print(1)\n",
    "    \"\"\"Function to build a model with specified layer dimensions and activation function.\"\"\"\n",
    "    model = Sequential()\n",
    "    for i, dim in enumerate(layer_dims):\n",
    "        if i == 0:\n",
    "            model.add(Conv2D(dim[0], dim[1], input_shape=input_shape, activation=activation))\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        else:\n",
    "            model.add(Conv2D(dim[0], dim[1], activation=activation))\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(filters=len_classes, kernel_size=1, strides=1))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(GlobalMaxPooling2D())\n",
    "    model.add(Activation('softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WMfe0TL_joQp"
   },
   "outputs": [],
   "source": [
    "def sample_architecture(min_layers=1, max_layers=5, min_filters=32, max_filters=512, min_kernel=3, max_kernel=5):\n",
    "    \"\"\"Function to sample a random architecture from the search space.\"\"\"\n",
    "    num_layers = random.randint(min_layers, max_layers)\n",
    "    layer_dims = [(random.randint(min_filters, max_filters), random.randint(min_kernel, max_kernel)) for _ in range(num_layers)]\n",
    "    return layer_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vekdUB7jjq2S"
   },
   "outputs": [],
   "source": [
    "def mutate_architecture(layer_dims, mutation_rate=0.1, min_layers=1, max_layers=5, min_filters=32, max_filters=512, min_kernel=3, max_kernel=5):\n",
    "    \"\"\"Function to mutate an architecture by randomly modifying some of its layer dimensions.\"\"\"\n",
    "    num_layers = len(layer_dims)\n",
    "    for i in range(num_layers):\n",
    "        if np.random.rand() < mutation_rate:\n",
    "            layer_dims[i] = (random.randint(min_filters, max_filters), random.randint(min_kernel, max_kernel))\n",
    "    return layer_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OBesHxjejvVo"
   },
   "outputs": [],
   "source": [
    "# def breed_architectures(parent1, parent2, mutation_rate=0.1, min_layers=1, max_layers=5, min_filters=32, max_filters=512, min_kernel=3, max_kernel=5):\n",
    "#     \"\"\"Function to breed two architectures to produce a new architecture.\"\"\"\n",
    "#     child = []\n",
    "#     for i in range(len(parent1)):\n",
    "#         if np.random.rand() < 0.5:\n",
    "#             child.append(parent1[i])\n",
    "#         else:\n",
    "#             child.append(parent2[i])\n",
    "#     return mutate_architecture(child, mutation_rate, min_layers, max_layers, \\\n",
    "#                                min_filters, max_filters, min_kernel, max_kernel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6-jGT9t8RZsa"
   },
   "outputs": [],
   "source": [
    "def breed_architectures(parent1, parent2, mutation_rate, min_layers, max_layers, min_filters, max_filters, min_kernel, max_kernel):\n",
    "    \"\"\"Function to breed two parent architectures to produce a child architecture.\"\"\"\n",
    "    child = []\n",
    "    for i in range(len(parent1)):\n",
    "        if np.random.rand() < 0.5:\n",
    "            child.append(parent1[i])\n",
    "        else:\n",
    "            if i < len(parent2):\n",
    "                child.append(parent2[i])\n",
    "    return mutate_architecture(child, mutation_rate, min_layers, max_layers, min_filters, max_filters, min_kernel, max_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "UiCgtQa9-62a"
   },
   "outputs": [],
   "source": [
    "def get_data(train_dir,validation_dir,batch_size=60,test_train_split=0.2):\n",
    "    train_data = image_dataset_from_directory(\\\n",
    "      train_dir,color_mode=\"grayscale\",image_size=(256,256) ,\\\n",
    "      subset='training',seed=12, validation_split=test_train_split,\\\n",
    "      batch_size=batch_size)\n",
    "    validation_data = image_dataset_from_directory(validation_dir,\n",
    "      color_mode=\"grayscale\",image_size=(256,256), subset='validation',seed=12,\\\n",
    "      validation_split=test_train_split,batch_size=batch_size)\n",
    "    return train_data,validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "lbnz3OeokBgR"
   },
   "outputs": [],
   "source": [
    "#Train the model on the dataset and evaluate its performance.\n",
    "def train_and_evaluate_model(model,epochs=10, train_dir=None,X_train=None, y_train=None,\\\n",
    "                             X_test=None,y_test=None):\n",
    "    \"\"\"Function to train and evaluate a model.\"\"\"\n",
    "    if train_dir is not None:\n",
    "        train_data,validation_data=get_data(train_dir,train_dir)\n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        history = model.fit(train_data, epochs=epochs, verbose=1,validation_data=validation_data)\n",
    "        # return history\n",
    "        # Extract the accuracy from the history object\n",
    "        acc = history.history['val_accuracy'][len(history.history['val_accuracy'])-1]\n",
    "        return acc\n",
    "    else:\n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        model.fit(X_train, y_train, epochs=epochs, batch_size=32, verbose=1)\n",
    "        scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "        return scores[1]  # Return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2zwUTo9ykIp7"
   },
   "outputs": [],
   "source": [
    "# 4) Define the genetic algorithm to evolve the architecture.\n",
    "def genetic_algorithm(train_dir,epochs,population_size=20,len_classes=3, num_generations=10, mutation_rate=0.1,\\\n",
    "                      min_layers=1, max_layers=5, min_filters=32, max_filters=512,\\\n",
    "                      min_kernel=3, max_kernel=5):\n",
    "    \"\"\"Function to implement the genetic algorithm to evolve the architecture.\"\"\"\n",
    "    # Initialize population\n",
    "    population = [sample_architecture(min_layers, max_layers, min_filters,\\\n",
    "                                      max_filters,min_kernel, max_kernel)\\\n",
    "                  for _ in range(population_size)]\n",
    "    scores = []\n",
    "    for i in range(num_generations):\n",
    "        print(\"Generation {}/{}\".format(i + 1, num_generations))\n",
    "        new_population = []\n",
    "        for j in range(population_size):\n",
    "            # Select two random parents\n",
    "            the_choice=np.random.choice(population_size, 2, replace=False)\n",
    "            parent1=population[the_choice[0]]\n",
    "            parent2=population[the_choice[1]]\n",
    "            # Breed the parents to produce a child\n",
    "            child = breed_architectures(parent1, parent2, mutation_rate,\\\n",
    "                                        min_layers,max_layers, min_filters,\\\n",
    "                                        max_filters, min_kernel, max_kernel)\n",
    "            new_population.append(child)\n",
    "        # Evaluate each model in the population\n",
    "        scores = []\n",
    "        for j, layer_dims in enumerate(new_population):\n",
    "            model = build_model(layer_dims,len_classes=len_classes,input_shape=(256,256,1,))\n",
    "            # (X_train, y_train), (X_test, y_test)=tf.keras.datasets.mnist.load_data()\n",
    "            score = train_and_evaluate_model(model,epochs=epochs,train_dir=train_dir)\n",
    "#  X_train, y_train, X_test, y_test)\n",
    "            scores.append((layer_dims, score))\n",
    "        # Sort the models by accuracy and select the best ones for the next generation\n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        population = [x[0] for x in scores[:population_size]]\n",
    "    # Return the best architecture\n",
    "    return population[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "1fn6mVEWSatF"
   },
   "outputs": [],
   "source": [
    "# train_dir=\"/home/shubhransu_etc/project_sys22/data/mammogram\"\n",
    "# test_dir=\"/content/drive/MyDrive/cbis_ddsm/mass/test\"\n",
    "train_dir=\"/home/shubhransu_etc/project_sys22/data/thermogram/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "J5rKCnQajbbI",
    "outputId": "7ef537d1-5197-44a4-d0f6-25104d0aa12c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1/30\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 13:30:53.821714: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-06 13:30:53.821831: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-06 13:30:53.821875: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (paramshavak-CELSIUS-R940): /proc/driver/nvidia/version does not exist\n",
      "2023-02-06 13:30:53.822475: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1282 files belonging to 2 classes.\n",
      "Using 1026 files for training.\n",
      "Found 1282 files belonging to 2 classes.\n",
      "Using 256 files for validation.\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 13:30:57.842400: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 2616768960 exceeds 10% of free system memory.\n",
      "2023-02-06 13:31:14.957094: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 2616768960 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/18 [>.............................] - ETA: 7:53 - loss: 1.5512 - accuracy: 0.5667"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 13:31:22.999138: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 2616768960 exceeds 10% of free system memory.\n",
      "2023-02-06 13:31:38.021118: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 2616768960 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2/18 [==>...........................] - ETA: 5:49 - loss: 1.1174 - accuracy: 0.5583"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 13:31:44.831890: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 2616768960 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 419s 23s/step - loss: 0.7373 - accuracy: 0.5224 - val_loss: 0.7008 - val_accuracy: 0.5117\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 407s 23s/step - loss: 0.6588 - accuracy: 0.6589 - val_loss: 0.6391 - val_accuracy: 0.6094\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 412s 23s/step - loss: 0.6234 - accuracy: 0.6959 - val_loss: 0.6390 - val_accuracy: 0.5898\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 408s 23s/step - loss: 0.5996 - accuracy: 0.6862 - val_loss: 0.6108 - val_accuracy: 0.6289\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 408s 23s/step - loss: 0.5813 - accuracy: 0.7281 - val_loss: 0.5921 - val_accuracy: 0.6562\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 408s 23s/step - loss: 0.5733 - accuracy: 0.7222 - val_loss: 0.5834 - val_accuracy: 0.6992\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 399s 22s/step - loss: 0.5632 - accuracy: 0.7427 - val_loss: 0.5904 - val_accuracy: 0.6562\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 400s 22s/step - loss: 0.5468 - accuracy: 0.7622 - val_loss: 0.5989 - val_accuracy: 0.6328\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 398s 22s/step - loss: 0.5335 - accuracy: 0.7680 - val_loss: 0.5792 - val_accuracy: 0.7148\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 405s 23s/step - loss: 0.5282 - accuracy: 0.7797 - val_loss: 0.5817 - val_accuracy: 0.7109\n",
      "1\n",
      "Found 1282 files belonging to 2 classes.\n",
      "Using 1026 files for training.\n",
      "Found 1282 files belonging to 2 classes.\n",
      "Using 256 files for validation.\n",
      "Epoch 1/10\n",
      "18/18 [==============================] - 500s 28s/step - loss: 0.7206 - accuracy: 0.4834 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 495s 27s/step - loss: 0.6931 - accuracy: 0.5049 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 491s 27s/step - loss: 0.6932 - accuracy: 0.5010 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 487s 27s/step - loss: 0.6933 - accuracy: 0.4815 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 489s 27s/step - loss: 0.6932 - accuracy: 0.4893 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 487s 27s/step - loss: 0.6932 - accuracy: 0.4951 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 487s 27s/step - loss: 0.6931 - accuracy: 0.5010 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 488s 27s/step - loss: 0.6933 - accuracy: 0.5010 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 487s 27s/step - loss: 0.6933 - accuracy: 0.5010 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 486s 27s/step - loss: 0.6932 - accuracy: 0.5010 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "1\n",
      "Found 1282 files belonging to 2 classes.\n",
      "Using 1026 files for training.\n",
      "Found 1282 files belonging to 2 classes.\n",
      "Using 256 files for validation.\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "# input_shape = (200, 100,3) \n",
    "# Run the genetic algorithm to evolve the architecture\n",
    "best_architecture = genetic_algorithm(train_dir=train_dir,len_classes=2,epochs=10, num_generations=30, mutation_rate=0.15,\\\n",
    "                      min_layers=1, max_layers=10, min_filters=32, max_filters=512,\\\n",
    "                      min_kernel=3, max_kernel=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model(input_shape=(224,224,3,),len_classes=3, dropout_rate=0.2,activation='relu'):\n",
    "#     \"\"\"Function to build a model with specified layer dimensions and activation function.\"\"\"\n",
    "#     model = Sequential()\n",
    "#     model.add(Conv2D(64, 3, input_shape=input_shape, activation=activation))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#     model.add(Conv2D(128, 3, activation=activation))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Conv2D(filters=len_classes, kernel_size=1, strides=1))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(GlobalMaxPooling2D())\n",
    "#     model.add(Activation('softmax'))\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=build_model(len_classes=3,input_shape=(256,256,1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# his=train_and_evaluate_model(model,epochs=2,train_dir=train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# his.history['val_accuracy'][len(his.history['val_accuracy'])-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6rtTbPp1jdY7"
   },
   "outputs": [],
   "source": [
    "# dir(keras.layers.sequential().fit)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
